Metadata-Version: 2.1
Name: neukolln
Version: 2.0.7
Summary: A set of extensions for the Scrapy framework. Why neukolln? Why not!
Home-page: https://github.com/AdeleH/neukolln
Author: ['Adele Harrissart', 'Baptiste Metge']
Author-email: ['adele.harrissart@gmail.com', 'baptiste.metge@gmail.com']
License: LICENSE.txt
Keywords: Scrapy Web Crawling
Platform: Any
Classifier: Development Status :: 2 - Pre-Alpha
Classifier: Environment :: Console
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Web Crawler
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 2
Classifier: Programming Language :: Python :: 2.7
Classifier: Topic :: Internet :: WWW/HTTP
Classifier: Topic :: Software Development :: Web Crawling
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Software Development :: Libraries :: Application Frameworks
Requires-Dist: Scrapy (>=1.4.0)
Requires-Dist: netifaces (==0.10.5)
Requires-Dist: phonenumbers (==8.3.2)
Requires-Dist: geopy (==1.11.0)
Requires-Dist: iso3166 (==0.8)
Requires-Dist: pycountry (==17.9.23)
Requires-Dist: nvector (==0.5.2)
Requires-Dist: PyMySQL (==0.7.11)
Requires-Dist: sshtunnel (==0.1.2)

# neukolln 2.0.7

A set of extensions for the framework Scrapy

![Alt text](https://media-cdn.holidaycheck.com/w_1280,h_720,c_fill,q_80/ugc/images/426298c6-4062-33e2-81a0-1910b1d9071c?raw=true "Neukölln")

## Platform (Python 2? 3?) and requirements 

Please check the `setup.py` file or print the help in command-line

```$> python setup.py --help-commands```

## Install the package using `pip`

`$> pip install git+ssh://git@GIT/neukolln.git@T28030v1.0.19`

or directly - within your `requirements.txt` file:

```
...
git+ssh://git@GIT/neukolln.git@T28030v1.0.20
...
```

where **1.0.18** is the name of the frozen version

Please check available branches: https://github.com/AdeleH/neukolln/branches

![neukolln](http://ljdchost.com/AGrn0cY.gif "python rocks")


## Main configuration


### Create your project via the **custom** command line

```
$> neukolln startproject myproject"
```

### [SETTINGS] In the `settings.py`, declare the custom pipeline and middlewares and add the proper configuration:

The custom pipeline allows **to store the data into different formats: JSON, CSV, TAB** (`CSV_JSON_TAB_Pipeline`) or **SQL + SSH** (`SSH_SQL_Pipeline`) 

The middlewares allows you:

- **to switch the user-agent or ip** (`RotateUserAgentAndIpAddressMiddleware`, `RotateIpAddressMiddleware`, `RandomUserAgentMiddleware`) [DOWNLOADER MIDDLEWARES]

- to have a custom policy **to refresh the cached page if needed** (`CustomPolicy`) [HTTPCACHE MIDDLEWARE]

=> please use the `NEUKOLLN_META_REFRESH_CACHE_KEY = "neukolln_refresh_cache"`

- **to customize the path to the cache with the date of the crawl** (`CustomFilesystemCacheStorage`) [HTTPCACHE MIDDLEWARE]

**This give you the possibility to re-start / re-do the parsing at a certain date by using the cache. [+++]**

e.g.

```
$> scrapy crawl myspider_spider -L INFO -a date="20170308"
```

```
BOT_NAME = 'myprojectname'  # required
FEED_EXPORT_FIELDS = [...]  # required

...

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

...

# Configure a delay for requests for the same website (default: 0)
# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
DOWNLOAD_DELAY = 0.25

...

DOWNLOADER_MIDDLEWARES = {
    'neukolln.middlewares.SmartRotateIpAddressMiddleware': 1,
    'neukolln.middlewares.RotateUserAgentAndIpAddressMiddleware': None,  # <==== deactivate
    'neukolln.middlewares.RotateIpAddressMiddleware': None,  # <==== deactivate
    'neukolln.middlewares.RandomUserAgentMiddleware': None,  # <==== deactivate
    #   '$project_name.middlewares.MyCustomDownloaderMiddleware': 543,
}

...

# Configure item pipelines
# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
    'neukolln.pipelines.CSV_JSON_TAB_Pipeline': 300,
}

...

HTTPCACHE_POLICY = 'neukolln.middlewares.CustomPolicy'  # scrapy.extensions.httpcache.DummyPolicy

...

# Enable and configure HTTP caching (disabled by default)
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
HTTPCACHE_ENABLED = True
HTTPCACHE_EXPIRATION_SECS = 24 * 3600  # Save file for one full day
HTTPCACHE_DIR = 'FR_MYPROJECT'  #  'httpcache'
HTTPCACHE_IGNORE_HTTP_CODES = [500, 503, 504, 400, 408, 404]  # Do not save 404 page...
HTTPCACHE_STORAGE = 'neukolln.middlewares.CustomFilesystemCacheStorage'  #  'scrapy.extensions.httpcache.FilesystemCacheStorage'
HTTPCACHE_IGNORE_MISSING = False  # If enabled, requests not found in the cache will be ignored instead of downloaded.

...

LOG_STDOUT = True
LOG_FILE = "myproject.log"
LOG_ENABLED = True
LOG_LEVEL = 'DEBUG'

...

# Set job directory to pause/stop and resume crawls
#  http://scrapy.readthedocs.io/en/latest/topics/jobs.html#job-directory
JOBDIR = 'jobs'

...

# Downloader middlewares
# Maximum number of times to retry, in addition to the first download.
RETRY_TIMES = 3  # Default: 2
RETRY_HTTP_CODES = [500, 503, 504, 400, 408, 404]  # Default: [500, 503, 504, 400, 408]
# Reminder:
# 400 Bad Request
# 401 Unauthorized
# 402 Payment Required
# 403 Forbidden
# 404 Not Found
# 405 Method Not Allowed
# 406 Not Acceptable
# 407 Proxy Authentication Required
# 408 Request Timeout
# 409 Conflict
# 410 Gone
# 411 Length Required
# 412 Precondition Failed
# 413 Request Entity Too Large
# 414 Request-URI Too Long
# 415 Unsupported Media Type
# 416 Requested Range Not Satisfiable
# 417 Expectation Failed
# 500 Internal Server Error
# 501 Not Implemented
# 502 Bad Gateway
# 503 Service Unavailable
# 504 Gateway Timeout
# 505 HTTP Version Not Supported

...

# https://doc.scrapy.org/en/latest/topics/feed-exports.html#feed-export-fields
# Use FEED_EXPORT_FIELDS option to define fields to export and their ORDER!!
FEED_EXPORT_FIELDS = [

    ...
    ...
    ...

]

FEED_EXPORT_DELIMITER = b';'  # default value
FEED_EXPORT_ENCODING = 'utf-8'  # default value

NEUKOLLN_PHONE_COMMON_DELIMITERS = ["-", ",", "_", ";", ":", "|", "/", "OU", " "]  # default values
NEUKOLLN_OUTPUTS_FOLDER = 'outputs'  # default value
```


### [ITEM] In your item:

The example of item is given here:

```
class NeukollnExampleItem(scrapy.Item):
    """
    Given as an example if you use the CSV_JSON_TAB_Pipeline
    """
    NEUKOLLN_ORIGINAL_PHONE_TELEPHONE = scrapy.Field(serializer=str)  # fill the original data here
    NEUKOLLN_DEFAULT_CC_TELEPHONE = scrapy.Field(serializer=str)  # precise the default code country here
    TELEPHONE = scrapy.Field(serializer=str)  # the "clean" parsed data will be set here by the pipeline if
```

```
import scrapy


class YourCustomProjectItem(scrapy.Item):
    ...
```

The fields whose prefix is `NEUKOLLN_ORIGINAL_PHONE_` are parsed / cleaned / and the phone numbers are detected by the `phonenumber` library.

**This has the advantage of not loosing the original data. [+++]**

In order to parse / clean / detect the phone numbers, the `phonenumber` library needs a default phone country given as
a parameter which should be set in the fields with the `NEUKOLLN_DEFAULT_CC_`.

Please check other specific items such as `ImmoItem`

```
class ImmoItem(NeukollnBaseItem, scrapy.Item):
    ...
```

=>

```
from neukolln.items import ImmoItem
...
item = ImmoItem()
...
item = set_basefields(spider, response)  # set base fields such as `ANNONCE_LINK`, `FROM_SITE`, `NEUKOLLN_FROM_LISTING_PAGE`, `NEUKOLLN_REQUEST_PATH`
```


### [SPIDER] Within the `spider` - you can use the `neukolln.spiders.NeukollnBaseSpider` - or configure your spider like below - in order to:

- set the date of the crawl by command line (which will be re-use both by the **CSV_JSON_TAB_Pipeline** to build the
path to the output files and by the **CustomFilesystemCacheStorage** to build a custom path to the cache with the date of the crawl)

- the data format(s) you need (**JSON is the default format.**)

- the middleware(s) you want to activate

```
class NeukollnBaseSpider(object):
    """Base Spider Class Used to Set / Store the Date of the Crawl."""

    # Date of the crawl
    crawl_date = None

    # Activate / Deactivate middlewares
    neukolln_rotate_ua_only = False  # rotate ip ONLY
    neukolln_rotate_ip_only = False  # rotate user-agent ONLY
    neukolln_rotate_ua_ip = False  # rotate user-agent AND ip
    neukolln_smart_rotate_ip = True  # "smart" rotate Ip

    neukolln_export_to_json = True  # default value is True
    neukolln_export_to_csv = True  # default value is False
    neukolln_export_to_tab = True  # default value is False

    ...

import scrapy
from neukolln.spiders import NeukollnBaseSpider


class MySpiderSpider(NeukollnBaseSpider, scrapy.Spider):
    name = "myspider_spider"
    allowed_domains = ["google.de"]
    start_urls = ['https://google.de/']

    # Keep only CSV
    neukolln_export_to_json = False  # default value is True
    neukolln_export_to_csv = True  # default value is False
    neukolln_export_to_tab = False  # default value is False

    def __init__(self, *args, **kwargs): # make sure your spider takes arguments if you need some (like "date" when using cache)

    ...
```

## Display in browser the cache html page

```neukolln html [path]``` where `path` is the path to the cache folder for the given crawled page


## REMINDER: About the Scrapy components:

- Scrapy Engine (it controls all components of the system)

- Scheduler (it receives the requests and enqueues them)

- Downloader (it fetches all web pages and feeds the spiders)

- Spiders (it parses the responses and extracts items)

- Item Pipeline (it processes the items, responsible of the **cleansing**, **validation** and **persistence**)

- Downloader middlewares (hooks between Engine and Downloader...)

- Spiders middlewares (hooks between Engine and Spider...)

![Alt text](https://doc.scrapy.org/en/latest/_images/scrapy_architecture_02.png?raw=true "Scrapy architecture and data flow (Source: https://doc.scrapy.org/en/)")




