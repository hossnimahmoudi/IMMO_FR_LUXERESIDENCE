#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Spider

from __future__ import unicode_literals

import time

from scrapy.utils.project import get_project_settings

import logging

logger = logging.getLogger()


class NeukollnBaseSpider(object):
    """Base Spider Class Used to Set / Store the Date of the Crawl."""

    # Date of the crawl
    crawl_date = None

    # Activate / Deactivate middlewares
    neukolln_rotate_ua_only = False  # rotate ip ONLY
    neukolln_rotate_ip_only = False  # rotate user-agent ONLY
    neukolln_rotate_ua_ip = False  # rotate user-agent AND ip
    neukolln_smart_rotate_ip = True  # "smart" rotate Ip

    neukolln_export_to_json = True  # default value is True
    neukolln_export_to_csv = True  # default value is False
    neukolln_export_to_tab = True  # default value is False

    def __init__(self, *args, **kwargs):
        """
        Pass a new argument (the date of the crawl) to the spider
        http://stackoverflow.com/questions/15611605/how-to-pass-a-user-defined-argument-in-scrapy-spider
        e.g. $> scrapy crawl my_spider -L INFO -a date="20170319"
        """
        super(NeukollnBaseSpider, self).__init__(*args, **kwargs)
        if hasattr(self, 'date'):
            self.crawl_date = self.date
        else:
            today = time.strftime("%Y%m%d")
            self.crawl_date = today

        # Alert user
        logger.info("[__INIT__][SPIDER][NEUKOLLN] Start crawling on %s" % self.crawl_date)

    def on_error(self, failed_response, item):
        """
        On error, return the item.
        Useful if you don't want to loose information / data / item if page request has failed.

        Use it in errback of the scrapy.Request as followed:

            scrapy.Request(url,
                ...
                errback=lambda failed_response, item=item: self.on_error(failure, item),
                ...
            )
        """
        settings = get_project_settings()

        # If the 'RetryMiddleware' is not allowed or the 'RetryMiddleware' is deactivated for the given request
        if not settings.get('RETRY_ENABLED', False) or failed_response.request.meta.get('dont_retry', False):
            # [ERROR] Alert user
            requested_url = failed_response.request.meta.get('redirect_urls', [failed_response.request.url])[
                0] if failed_response.request.meta else failed_response.request.url
            logger.error("[ON_ERROR][NEUKOLLN_BASE_SPIDER][NEUKOLLN] Request failed so yield item. "
                         "RETRY_ENABLED={0}, "
                         "dont_retry={1}, "
                         "requested url={2}, "
                         "item={3}".format(str(settings.get('RETRY_ENABLED', False)),
                                           str(failed_response.request.meta.get('dont_retry', False)),
                                           requested_url,
                                           str(item)))

            return item

        # RetryMiddleware
        #   https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.retry
        # "Failed pages are collected on the scraping process and rescheduled at the end, once the spider has finished
        # crawling all regular (non failed) pages."

        # Check that the request will NOT be processed again by the 'RetryMiddleware'
        if int(settings.get('RETRY_TIMES', 0)) <= failed_response.request.meta.get('retry_times', 0):
            # [ERROR] Alert user
            requested_url = failed_response.request.meta.get('redirect_urls', [failed_response.request.url])[
                0] if failed_response.request.meta else failed_response.request.url
            logger.error("[ON_ERROR][NEUKOLLN_BASE_SPIDER][NEUKOLLN] Request failed so yield item. "
                         "RETRY_TIMES={0}, "
                         "retry_times={1}, "
                         "requested url={2}, "
                         "item={3}".format(str(settings.get('RETRY_TIMES', 0)),
                                           str(failed_response.request.meta.get('retry_times', 0)),
                                           requested_url,
                                           str(item)))

            return item

            # In any other cases, the request will be processed again at the end =====> yield None

    def response_is_ban(self, request, response):
        """You can override or complete this method in your own spider if needed..."""
        ban = False

        # Check if there is a body
        bool = ((response.status == 200) and not len(response.body))
        if bool:
            logger.error("[RESPONSE_IS_BAN][NEUKOLLN_BASE_SPIDER][NEUKOLLN] response.status == 200 but body is empty")
        ban = ban or bool

        # Check the url
        bool = ("erreur" in response.url) or ("error" in response.url)
        if bool:
            logger.error("[RESPONSE_IS_BAN][NEUKOLLN_BASE_SPIDER][NEUKOLLN] \"error\" in url")
        ban = ban or bool

        # Check inside the body if "captcha" keyword
        try:
            bool = 'captcha' in response.body.lower()
        except UnicodeDecodeError:
            bool = 'captcha' in response.body.decode('latin-1').encode("utf-8").lower()
        if bool:
            logger.error("[RESPONSE_IS_BAN][NEUKOLLN_BASE_SPIDER][NEUKOLLN] \"captcha\" in url")
        ban = ban or bool

        # Inform user
        requested_url = request.meta.get('redirect_urls', [request.url])[0] if request.meta else request.url
        logger.debug("[RESPONSE_IS_BAN][NEUKOLLN_BASE_SPIDER][NEUKOLLN] "
                     "%s, response.status=%s, response.url=%s, requested url=%s"
                     % (str(ban), str(response.status), response.url, requested_url))

        # Return boolean
        return ban
