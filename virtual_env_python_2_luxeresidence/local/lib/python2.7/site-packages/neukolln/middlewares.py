#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Define here the models for your spider middleware
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/spider-middleware.html

from __future__ import unicode_literals

import os
import shutil

from utils import get_inet_ips
from utils import splash_custom_fingerprint
import random
from random import choice
import time
import ast, urllib2
from urlparse import urljoin

from scrapy.exceptions import IgnoreRequest

from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.downloadermiddlewares.redirect import RedirectMiddleware
from scrapy import signals
from scrapy.extensions.httpcache import FilesystemCacheStorage
from scrapy.utils.request import request_fingerprint
from scrapy.utils.httpobj import urlparse_cached
from scrapy.utils.project import get_project_settings
from scrapy.utils.response import response_status_message
from six.moves import cPickle as pickle
from w3lib.http import headers_raw_to_dict, headers_dict_to_raw
from scrapy.http import Headers, Response
from scrapy.responsetypes import responsetypes
from scrapy.utils.python import to_bytes, to_unicode


############### requets.py
import hashlib
import weakref
from six.moves.urllib.parse import urlunparse

from w3lib.http import basic_auth_header
from scrapy.utils.python import to_bytes, to_native_str

from w3lib.url import canonicalize_url
from scrapy.utils.httpobj import urlparse_cached


import logging

logger = logging.getLogger()

# SETTINGS
NEUKOLLN_SETTINGS_USER_AGENT_CHOICES = 'NEUKOLLN_USER_AGENT_CHOICES'

NEUKOLLN_META_REFRESH_CACHE_KEY = "neukolln_refresh_cache"
NEUKOLLN_META_REFRESH_CACHE_DEFAULT_VALUE = False

NEUKOLLN_META_BANNED_KEY = 'neukolln_banned'

# SPIDER
NEUKOLLN_SPIDER_ROTATE_IP_ONLY_ATTR = 'neukolln_rotate_ip_only'  # rotate ip ONLY
NEUKOLLN_SPIDER_ROTATE_UA_ONLY_ATTR = 'neukolln_rotate_ua_only'  # rotate user-agent ONLY
NEUKOLLN_SPIDER_ROTATE_IP_UA_ATTR = 'neukolln_rotate_ua_ip'  # rotate user-agent AND ip
NEUKOLLN_SPIDER_SMART_ROTATE_IP_ATTR = 'neukolln_smart_rotate_ip'

# Set of default user agent
# Define here a list you wanna use
# http://useragentstring.com/pages/useragentstring.php
# Or fake it: https://pypi.python.org/pypi/fake-useragent
NEUKOLLN_USER_AGENT_DEFAULT_LIST = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8',
    'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64; rv:45.0) Gecko/20100101 Firefox/45.0',
    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (Windows NT 6.3; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (Windows NT 6.1; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (X11; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/602.3.12 (KHTML, like Gecko) Version/10.0.2 Safari/602.3.12',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0',
    'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8',
    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:45.0) Gecko/20100101 Firefox/45.0',
    'Mozilla/5.0 (iPad; CPU OS 10_2_1 like Mac OS X) AppleWebKit/602.4.6 (KHTML, like Gecko) Version/10.0 Mobile/14D27 Safari/602.1',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/55.0.2883.87 Chrome/55.0.2883.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0',
    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 10_2_1 like Mac OS X) AppleWebKit/602.4.6 (KHTML, like Gecko) Version/10.0 Mobile/14D27 Safari/602.1',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.7 (KHTML, like Gecko) Version/9.1.2 Safari/601.7.7',
    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0;  Trident/5.0)',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 10_2_1 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/56.0.2924.79 Mobile/14D27 Safari/602.1',
    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;  Trident/5.0)',
    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.73 Safari/537.36 OPR/34.0.2036.25',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/56.0.2924.76 Chrome/56.0.2924.76 Safari/537.36',
    'Mozilla/5.0 (Windows NT 5.1; rv:51.0) Gecko/20100101 Firefox/51.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.100 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12) AppleWebKit/602.1.50 (KHTML, like Gecko) Version/10.0 Safari/602.1.50',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2486.0 Safari/537.36 Edge/13.10586',
    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:52.0) Gecko/20100101 Firefox/52.0',
    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 OPR/43.0.2442.1144',
    'Mozilla/5.0 (Windows NT 6.1; rv:45.0) Gecko/20100101 Firefox/45.0',
    'Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/601.6.17 (KHTML, like Gecko) Version/9.1.1 Safari/601.6.17',
    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 OPR/43.0.2442.1144',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.75 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'
]


class CustomFilesystemCacheStorage(FilesystemCacheStorage):
    """
    Custom File System Cache Storage

    Extension of the 'scrapy.extensions.httpcache import FilesystemCacheStorage'
        https://github.com/scrapy/scrapy/blob/master/scrapy/extensions/httpcache.py

    =======> Use the date of the crawl to customize the path to the cache <=======
    """

    def __init__(self, settings):
        super(CustomFilesystemCacheStorage, self).__init__(settings)
        # Define a new attribute here: the date of the crawl
        self.crawl_date = None

    def open_spider(self, spider):
        if hasattr(spider, 'date'):
            # Get the value from the spider
            self.crawl_date = spider.date
        else:
            # Set it by default with the date of crawl launch
            today = time.strftime("%Y%m%d")
            self.crawl_date = today
        logger.debug("[OPEN_SPIDER][CUSTOM_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] Using filesystem cache storage located "
                     "in %(cachedir)s/%(crawl_date)s" % {'cachedir': self.cachedir,
                                                         'crawl_date': self.crawl_date}, extra={'spider': spider})

    def close_spider(self, spider):
        pass

    def _get_request_path(self, spider, request):
        """Return the path to the folder"""
        if 'splash' not in request.meta.keys():
            key = request_fingerprint(request)
        if 'splash' in request.meta.keys():
            key= splash_custom_fingerprint(request)
        # Build a path with the date of the crawl (from the spider) instead of the name of the spider
        return os.path.join(self.cachedir, self.crawl_date, key[0:2], key)


class CustomPhoneFilesystemCacheStorage(FilesystemCacheStorage):
    """
    Custom Phone File System Cache Storage. Useful if you call an API for phone and want a dedicated cache folder for the phone API calls

    Extension of the 'scrapy.extensions.httpcache import FilesystemCacheStorage'
        https://github.com/scrapy/scrapy/blob/master/scrapy/extensions/httpcache.py

    =======> Use a PHONES folder for phone if request.meta['phone_specific_cache'] == True <==========
    =======> Use the date folder otherwise <=========
    """

    def __init__(self, settings):
        super(CustomPhoneFilesystemCacheStorage, self).__init__(settings)
        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')
        self.expiration_phones_secs = settings.getint('HTTPCACHE_PHONES_EXPIRATION_SECS')
        # Define a new attribute here: the date of the crawl
        self.crawl_date = None

    def open_spider(self, spider):
        if hasattr(spider, 'date'):
            # Get the value from the spider
            self.crawl_date = spider.date
        else:
            # Set it by default with the date of crawl launch
            today = time.strftime("%Y%m%d")
            self.crawl_date = today
        logger.debug("[OPEN_SPIDER][CUSTOM_PHONES_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] Using filesystem cache storage located "
                     "in %(cachedir)s/%(crawl_date)s and %(cachedir)s/PHONES" % {'cachedir': self.cachedir,
                                                         'crawl_date': self.crawl_date}, extra={'spider': spider})

    def close_spider(self, spider):
        pass

    def _get_request_path(self, spider, request):
        """Return the path to the folder PHONES if phone_specific_cache enable , and path with the date of the crawl if not """
        key = request_fingerprint(request)
        # check if phone specific cache system is required
        if 'phone_specific_cache' in request.meta.keys():
            print(request.meta["phone_specific_cache"])
            return os.path.join(self.cachedir, "PHONES", key[0:2], key)
        # Build a path with the date of the crawl (from the spider) instead of the name of the spider
        else:
            return os.path.join(self.cachedir, self.crawl_date, key[0:2], key)

    def retrieve_response(self, spider, request):
        """Return response if present in cache, or None otherwise."""
        metadata = self._read_meta(spider, request)
        if metadata is None:
            return  # not cached
        rpath = self._get_request_path(spider, request)
        with self._open(os.path.join(rpath, 'response_body'), 'rb') as f:
            body = f.read()
        with self._open(os.path.join(rpath, 'response_headers'), 'rb') as f:
            rawheaders = f.read()
        url = metadata.get('response_url')
        status = metadata['status']
        headers = Headers(headers_raw_to_dict(rawheaders))
        respcls = responsetypes.from_args(headers=headers, url=url)
        response = respcls(url=url, headers=headers, status=status, body=body)
        return response

    # to determine if cache as expired
    def _read_meta(self, spider, request):
        rpath = self._get_request_path(spider, request)
        metapath = os.path.join(rpath, 'pickled_meta')
        if not os.path.exists(metapath):
            return  # not found
        mtime = os.stat(metapath).st_mtime
        try:
            if request.meta["phone_specific_cache"]:
                if 0 < self.expiration_phones_secs < time.time() - mtime:
                    print('[custom_phone_cache_middleware] updating_phone_expired ===> ' + str(request.url))
                    return  # phone expired
            else:
                if 0 < self.expiration_secs < time.time() - mtime:
                    print('[custom_phone_cache_middleware] updating_page_expired ===> ' + str(request.url))
                    return  # page expired
            with self._open(metapath, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            if 0 < self.expiration_secs < time.time() - mtime:
                print('[custom_phone_cache_middleware] updating_page_expired ===> ' + str(request.url))
                return  # page expired
            with self._open(metapath, 'rb') as f:
                return pickle.load(f)

###########################
#### TODO : 1) using splash_request_finguerprint instead of check if splash activated
####        2) What if we re-run on cache from 20171201 but they was a crawl meanwhile the 20180101 , so most recent cache detected will be the last one , but we don't won't it as we try to re-run on the status of 20171201 ?
####        3) avoid function _clean_banned_cache 
###########################


class CustomDeltaFilesystemCacheStorage(FilesystemCacheStorage):
    """
    Custom Phone File System Cache Storage. Useful if you call an API for phone and want a dedicated cache folder for the phone API calls

    Extension of the 'scrapy.extensions.httpcache import FilesystemCacheStorage'
        https://github.com/scrapy/scrapy/blob/master/scrapy/extensions/httpcache.py

    =======> Use a PHONES folder for phone if request.meta['phone_specific_cache'] == True <==========
    =======> Use the date folder otherwise <=========
    """

    def __init__(self, settings):
        super(CustomDeltaFilesystemCacheStorage, self).__init__(settings)
        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')
        self.expiration_delta_secs = settings.getint('HTTPCACHE_DELTA_EXPIRATION_SECS')
        self.expiration_phones_secs = settings.getint('HTTPCACHE_PHONES_EXPIRATION_SECS')
        self.crawl_date = None

    def open_spider(self, spider):
        if hasattr(spider, 'date'):
            # Get the value from the spider
            self.crawl_date = spider.date
        else:
            # Set it by default with the date of crawl launch
            today = time.strftime("%Y%m%d")
            self.crawl_date = today
        logger.debug("[OPEN_SPIDER][CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] Using filesystem cache storage located "
                     "in %(cachedir)s/%(crawl_date)s" % {'cachedir': self.cachedir,
                                                         'crawl_date': self.crawl_date}, extra={'spider': spider})
        logger.debug("[OPEN_SPIDER][CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] Settings : self.expiration_secs = " + str(self.expiration_secs)) 
        logger.debug("[OPEN_SPIDER][CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] Settings : self.expiration_delta_secs = " + str(self.expiration_delta_secs)) 
        logger.debug("[OPEN_SPIDER][CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] Settings : self.expiration_phones_secs = " + str(self.expiration_phones_secs)) 

    def close_spider(self, spider):
        pass

    def store_response(self, spider, request, response):
        """Store the given response in the cache."""
        rpath = self._get_request_path(spider, request) # we need tha path of the crawl date folder to store it, as we will store the reponse only if it was not retrieve in some previous cache
        if not os.path.exists(rpath):
            os.makedirs(rpath)
        metadata = {
            'url': request.url,
            'method': request.method,
            'status': response.status,
            'response_url': response.url,
            'timestamp': time.time(),
        }
        if 'delta_matching_json' in request.meta.keys():
            metadata['delta_matching_json'] = request.meta['delta_matching_json']
        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:
            f.write(to_bytes(repr(metadata)))
        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:
            pickle.dump(metadata, f, protocol=2)
        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:
            f.write(headers_dict_to_raw(response.headers))
        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:
            f.write(response.body)
        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:
            f.write(headers_dict_to_raw(request.headers))
        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:
            f.write(request.body)

    def retrieve_response(self, spider, request):
        """Return response if present in cache, or None otherwise."""
        if "neukolln_banned" in request.meta.keys():
            print('[CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN]SHOULD REFRESH CACHE')
            self._clean_banned_cache(spider, request)
        metadata = self._read_meta(spider, request)
        if metadata is None:
            return  # not cached
        if 'delta_matching_json' in request.meta.keys():
            rpath = self._get_delta_request_path(spider, request)
        else:
            rpath = self._get_request_path(spider, request)
        with self._open(os.path.join(rpath, 'response_body'), 'rb') as f:
            body = f.read()
        with self._open(os.path.join(rpath, 'response_headers'), 'rb') as f:
            rawheaders = f.read()
        url = metadata.get('response_url')
        status = metadata['status']
        headers = Headers(headers_raw_to_dict(rawheaders))
        respcls = responsetypes.from_args(headers=headers, url=url)
        response = respcls(url=url, headers=headers, status=status, body=body)
        return response
        #return None


    # to determine if cache as expired or if delta doesn't match
    def _read_meta(self, spider, request):
        # if ban ==> should redownload
        if 'delta_matching_json' in request.meta.keys():
            rpath = self._get_delta_request_path(spider, request)
        else:
            rpath = self._get_request_path(spider, request)
        metapath = os.path.join(rpath, 'pickled_meta')
        if not os.path.exists(metapath):
            return  # not found
        ########################
        # Some cache was found #
        ########################
        mtime = os.stat(metapath).st_mtime
        # If phone_specific_cache enable, check if phone cache as expired
        if 'phone_specific_cache' in request.meta.keys():
            if 0 < self.expiration_phones_secs < time.time() - mtime:
                print('[CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] phone page expired , updating it ===> ' + str(request.url))
                return  # phone expired
        # If delta applicable, check if as expired cache as expired
        if 'delta_matching_json' in request.meta.keys():
            # checl delta expiration was not overpassed
            if 0 < self.expiration_delta_secs < time.time() - mtime:
                print('[CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] page expired , updating it ===> ' + str(request.url))
                return  # page expired
            else:
                try:
                    with self._open(metapath, 'rb') as f:
                        print('[CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] comparing keys')
                        dict_1 = request.meta['delta_matching_json']
                        dict_2 = pickle.load(f)['delta_matching_json']
                        print(dict_1)
                        print(dict_2)
                        if (dict_1 == dict_2):
                            with self._open(metapath, 'rb') as f:
                                print('[OPEN_SPIDER][CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] : delta is null => take page from cache')
                                return pickle.load(f)
                        else:
                            print('[OPEN_SPIDER][CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] : delta not null => downloading page')
                            return
                except Exception as e:
                    print('[OPEN_SPIDER][CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] errors comparing keys===> ' + str(e))
                    return
        # no delta_matching_json => delta not applicable on this page => we download page from the date
        else:
            if 0 < self.expiration_secs < time.time() - mtime:
                print('[OPEN_SPIDER][CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] page expired , updating it ===> ' + str(request.url))
                return  # page expired
            else:
                with self._open(metapath, 'rb') as f:
                    print('[OPEN_SPIDER][CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] returning standard cache')
                    return pickle.load(f)

    def _clean_banned_cache(self, spider, request):
        try:
            key = self._delta_request_fingerprint(request)
            cache_folders = next(os.walk(self.cachedir))[1]
            possible_folders =  set()
            for folder in cache_folders:
                if os.path.isdir(os.path.join(self.cachedir, folder, key[0:2], key)):
                    possible_folders.add(os.path.join(self.cachedir, folder, key[0:2], key))
                    shutil.rmtree(os.path.join(self.cachedir, folder, key[0:2], key))
            print("[CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE] _clean_banned_cache => ")
            print(possible_folders)
        except Exception as e:
            print('error '+ str(e))



    def _get_delta_request_path(self, spider, request):
        """If any : Return the path to the cache folder containing the most recent version of the request"""
        key = self._delta_request_fingerprint(request)
        try:
            cache_folders = next(os.walk(self.cachedir))[1]
            possible_folders =  set()
            for folder in cache_folders:
                if os.path.isdir(os.path.join(self.cachedir, folder, key[0:2], key)):
                    possible_folders.add(os.path.join(self.cachedir, folder, key[0:2], key))
            most_recent_folder = max(possible_folders, key=os.path.getmtime)
            print("[CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] most recent cache folder containing the request => " + most_recent_folder)
            return most_recent_folder
        except:
            print('[CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] no delta folder containing cache requests => therefore will be cache in => ' + os.path.join(self.cachedir, self.crawl_date, key[0:2], key))
            return os.path.join(self.cachedir, self.crawl_date, key[0:2], key)

    def _get_request_path(self, spider, request):
        """Return the path to the folder PHONES if phone_specific_cache enable , and path with the date of the crawl if not """
        key = self._delta_request_fingerprint(request)
        # check if phone specific cache system is required
        if 'phone_specific_cache' in request.meta.keys():
            print(request.meta["phone_specific_cache"])
            return os.path.join(self.cachedir, "PHONES", key[0:2], key)
        # Build a path with the date of the crawl (from the spider) instead of the name of the spider
        else:
            print('[OPEN_SPIDER][CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] no folder containing cache requests => therefore will be cache in => ' + os.path.join(self.cachedir, self.crawl_date, key[0:2], key))
            return os.path.join(self.cachedir, self.crawl_date, key[0:2], key)

    def _delta_request_fingerprint(self, request, include_headers=None):
        if 'splash' not in request.meta.keys():
            print('[CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] no_splash')
            print(request.url)
            key = request_fingerprint(request)
            return key
        if 'splash' in request.meta.keys():
            print('[CUSTOM_DELTA_FILESYSTEM_CACHE_STORAGE][NEUKOLLN] with_splash')
            print(request.url)

            if include_headers:
                include_headers = tuple(to_bytes(h.lower())
                                         for h in sorted(include_headers))
            #_fingerprint_cache = weakref.WeakKeyDictionary()
            cache = weakref.WeakKeyDictionary().setdefault(request, {})
            if include_headers not in cache:
                fp = hashlib.sha1()
                fp.update(to_bytes("GET"))#request.method))
                fp.update(to_bytes(canonicalize_url(request.meta['splash']['args']['url'])))
                fp.update(b'')#request.body or b'')
                if include_headers:
                    for hdr in include_headers:
                        if hdr in request.headers:
                            fp.update(hdr)
                            for v in request.headers.getlist(hdr):
                                fp.update(v)
                cache[include_headers] = fp.hexdigest()
            return cache[include_headers]

###################
# To be completed #
###################
class CustomNoIdclientDeltaFilesystemCacheStorage(FilesystemCacheStorage):
    """
    Custom Delta Middleware creating a kind of hash key based but on the values in delta_matching_json. 
    Uselful if you want to avoid having a cache based on URL as it is in default in scrapy-how-to-remove-a-url-from-httpcache-or-prevent-adding-to-cache
    ===> this can be problematic when URL contains changing ID_CLIENT for exemple


    """

    def _custom_fingerprint(self, request, include_headers=None):
        print('new_finger_print')
        print(request.meta['delta_matching_json'])
        finger_print = ""
        for delta_key in request.meta['delta_matching_json']:
            finger_print =  finger_print + request.meta['delta_matching_json'][delta_key] + "/"
        print(finger_print)
        return finger_print


class NeukollnBaseMiddleware(object):
    """Mandatory class to connect middleware when spider is open (signal)"""

    @classmethod
    def from_crawler(cls, crawler):
        o = cls()
        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)  # <============== mandatory!!!
        return o


class RotateIpAddressMiddleware(NeukollnBaseMiddleware):
    """Rotate ip address for each request."""

    def __init__(self):
        self.enabled = False

        ips = get_inet_ips()
        ips = ips if ips else ['0.0.0.0']
        self.ip_addresses = ips

    def spider_opened(self, spider):
        self.enabled = getattr(spider, NEUKOLLN_SPIDER_ROTATE_IP_ONLY_ATTR, self.enabled)
        if self.enabled:
            if getattr(spider, NEUKOLLN_SPIDER_ROTATE_IP_UA_ATTR, False):
                raise Exception("You can not activate both RotateIpAddressMiddleware AND "
                                "RotateUserAgentAndIpAddressMiddleware! Please check the spider attributes.")

    def process_request(self, request, spider):
        """Set a random IP"""
        if self.enabled:
            ip_address = random.choice(self.ip_addresses)
            request.meta['bindaddress'] = (ip_address, 0)

            # [INFO] Alert user
            logger.info("[ROTATE_IP_MIDDLEWARE][PROCESS_REQUEST][NEUKOLLN] My IP is ==========> "
                        + str(request.meta['bindaddress']))


class RandomUserAgentMiddleware(object):
    """Rotate user-agent for each request."""

    def __init__(self, user_agents):
        self.enabled = False
        self.user_agents = user_agents

    @classmethod
    def from_crawler(cls, crawler):
        user_agents = get_project_settings().get(NEUKOLLN_SETTINGS_USER_AGENT_CHOICES)
        o = cls(user_agents)
        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
        return o

    def spider_opened(self, spider):
        self.enabled = getattr(spider, NEUKOLLN_SPIDER_ROTATE_UA_ONLY_ATTR, self.enabled)
        if self.enabled:
            if not self.user_agents:
                # [DEBUG] Alert user
                logger.debug("[ROTATE_UA_MIDDLEWARE][SPIDER_OPENED][NEUKOLLN] %s is not set in the settings so a "
                             "default list of user agents from the library will be used."
                             % NEUKOLLN_SETTINGS_USER_AGENT_CHOICES)
                self.user_agents = NEUKOLLN_USER_AGENT_DEFAULT_LIST
            if getattr(spider, NEUKOLLN_SPIDER_ROTATE_IP_UA_ATTR, False):
                raise Exception("You can not activate both RandomUserAgentMiddleware AND "
                                "RotateUserAgentAndIpAddressMiddleware! Please check the spider attributes.")

    def process_request(self, request, spider):
        """Set a random user-agent from list"""
        if self.enabled and self.user_agents:
            request.headers['user-agent'] = choice(self.user_agents)

            # [INFO] Alert user
            logger.info("[ROTATE_UA_MIDDLEWARE][PROCESS_REQUEST][NEUKOLLN] My USER AGENT is ==========> "
                        + str(request.headers['user-agent']))


class RotateUserAgentAndIpAddressMiddleware(NeukollnBaseMiddleware):
    """Rotate ip address and user-agent as well for each request. Keep the same user-agent for each IP!"""

    def __init__(self):
        self.enabled = False
        self.user_agents = get_project_settings().get(NEUKOLLN_SETTINGS_USER_AGENT_CHOICES)
        ips = get_inet_ips()
        self.ips = ips if ips else ['0.0.0.0']
        self.ip_addresses = self.ips

    def spider_opened(self, spider):
        self.enabled = getattr(spider, NEUKOLLN_SPIDER_ROTATE_IP_UA_ATTR, self.enabled)
        if self.enabled:
            if not self.user_agents:
                # [DEBUG] Alert user
                logger.debug(
                    "[ROTATE_UA_AND_IP_MIDDLEWARE][SPIDER_OPENED][NEUKOLLN] %s is not set in the settings so a "
                    "default list of user agents from the library will be used."
                    % NEUKOLLN_SETTINGS_USER_AGENT_CHOICES)
                self.user_agents = NEUKOLLN_USER_AGENT_DEFAULT_LIST
            if getattr(spider, NEUKOLLN_SPIDER_ROTATE_IP_ONLY_ATTR, False):
                raise Exception("You can not activate both RotateIpAddressMiddleware AND "
                                "RotateUserAgentAndIpAddressMiddleware! Please check the spider attributes.")
            if getattr(spider, NEUKOLLN_SPIDER_ROTATE_UA_ONLY_ATTR, False):
                raise Exception("You can not activate both RandomUserAgentMiddleware AND "
                                "RotateUserAgentAndIpAddressMiddleware! Please check the spider attributes.")

    def process_request(self, request, spider):
        """Set a random IP. Keep the same user-agent for each IP!"""
        if self.enabled:
            i = random.choice(range(0, len(self.ip_addresses)))
            ip_address = self.ip_addresses[i]
            request.meta['bindaddress'] = (ip_address, 0)
            # Keep the same user-agent for each IP!
            j = i % (len(self.user_agents) - 1)
            request.headers['user-agent'] = self.user_agents[j]

            # [INFO] Alert user
            logger.info("[ROTATE_UA_AND_IP_MIDDLEWARE][PROCESS_REQUEST][NEUKOLLN] My IP is ==========> "
                        + str(request.meta['bindaddress']))
            logger.info("[ROTATE_UA_AND_IP_MIDDLEWARE][PROCESS_REQUEST][NEUKOLLN] My USER AGENT is ==========> "
                        + str(request.headers['user-agent']))


class CustomPolicy(object):
    """
    Custom Policy Middleware

    Have a look at the DummyPolicy middleware in Scrapy:
        https://github.com/scrapy/scrapy/blob/master/scrapy/extensions/httpcache.py

    Have a look at the post from Stackoverflow under the following topic:
        "Scrapy how to remove a url from httpcache or prevent adding to cache"
        http://stackoverflow.com/questions/41743071/scrapy-how-to-remove-a-url-from-httpcache-or-prevent-adding-to-cache
    """

    def __init__(self, settings):
        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')
        self.ignore_http_codes = [int(x) for x in settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES')]

    def should_cache_request(self, request):
        return urlparse_cached(request).scheme not in self.ignore_schemes

    def should_cache_response(self, response, request):
        bool = response.status not in self.ignore_http_codes
        # required_xpath = request.meta.get(NEUKOLLN_META_REQUIRED_XPATH_KEY, None)
        # if bool and required_xpath:
        #     # If there is a response but not the required xpath,
        #     # it means that something went probably wrong...so do NOT cache the page
        #     required_el = response.xpath(required_xpath)
        #     if not required_el or not required_el.extract_first():
        #         return False
        return bool

    def is_cached_response_fresh(self, response, request):
        """
        If it is NOT ask to refresh the cache,
        then the response is already considered as fresh
        and does not need to be refresh
        """
        return not request.meta.get(NEUKOLLN_META_REFRESH_CACHE_KEY, NEUKOLLN_META_REFRESH_CACHE_DEFAULT_VALUE)

    def is_cached_response_valid(self, cachedresponse, response, request):
        bool = not (request.meta.get(NEUKOLLN_META_REFRESH_CACHE_KEY, NEUKOLLN_META_REFRESH_CACHE_DEFAULT_VALUE))
        if not bool:
            # [INFO] Alert user using debug logger
            logger.info("[CUSTOM_POLICY_MIDDLEWARE][HTTPCACHE_POLICY][NEUKOLLN] ==========>  refreshing the cache ")
        return bool


###
# The idea of the SmartRotateIpAddressMiddleware comes from the "scrapy-rotating-proxies"
# =====> https://github.com/TeamHG-Memex/scrapy-rotating-proxies/blob/master/rotating_proxies/middlewares.py
###


class SmartRotateIpAddressMiddleware(NeukollnBaseMiddleware):
    """
    Change Ip if just banned
    """

    def __init__(self):
        self.enabled = False  # disable by default

        # Set list of user agent
        self.user_agents = get_project_settings().get(NEUKOLLN_SETTINGS_USER_AGENT_CHOICES)
        if not self.user_agents:
            # [DEBUG] Alert user
            logger.debug(
                "[SMART_ROTATE_IP_MIDDLEWARE][__INIT__][NEUKOLLN] %s is not set in the settings so a "
                "default list of user agents from the library will be used."
                % NEUKOLLN_SETTINGS_USER_AGENT_CHOICES)
            self.user_agents = NEUKOLLN_USER_AGENT_DEFAULT_LIST

        # Get available ips
        ips = get_inet_ips()
        self.ips = ips if ips else ['0.0.0.0']

        # [INFO] Alert user
        logger.info("[SMART_ROTATE_IP_MIDDLEWARE][__INIT__][NEUKOLLN] Available ips: %s" % ", ".join(self.ips))

        # [INITIALIZATION]
        self.pool = list(self.ips)  # fill pool with Ip addresses ======> copy!!!
        self.reset_count = 0  # number of times the pool has been reset

    def get_random_ip_from_pool(self):
        """internal util function"""
        return random.choice(self.pool)

    def get_user_agent_for_ip(self, ip):
        i = self.ips.index(ip)
        j = i % len(self.user_agents)
        return self.user_agents[j]

    def switch_ip(self, banned_ip, banned_url, METHOD):
        """internal util function"""

        # Remove ip from pool
        if banned_ip in self.pool:
            self.pool.remove(banned_ip)

        # Check if pool is empty
        if len(self.pool) == 0:
            # [RESET]
            self.pool = list(self.ips)  # fill again the pool with all available Ips ======> copy!!!
            self.reset_count += 1

            # [DEBUG] Alert user that Ip address has been banned
            seconds = 3600 * self.reset_count
            logger.debug("[SMART_ROTATE_IP_MIDDLEWARE][%s][NEUKOLLN] Pool of Ips has been reset "
                         "for %d the time. Wait %s seconds" % (METHOD, self.reset_count, seconds))
            time.sleep(seconds)

        # [DEBUG] Alert user that Ip address has been banned
        logger.debug("[SMART_ROTATE_IP_MIDDLEWARE][%s][NEUKOLLN] %s has been banned while trying to access to %s.\n"
                     "Switch with an other random available ip.\nCarefull %d ips only left over %d!"
                     % (METHOD,
                        banned_ip,
                        banned_url,
                        len(self.pool),
                        len(self.ips)))

    def spider_opened(self, spider):
        self.enabled = getattr(spider, NEUKOLLN_SPIDER_SMART_ROTATE_IP_ATTR, self.enabled)
        if self.enabled:
            if getattr(spider, NEUKOLLN_SPIDER_ROTATE_IP_UA_ATTR, False) or getattr(spider,
                                                                                    NEUKOLLN_SPIDER_ROTATE_IP_ONLY_ATTR,
                                                                                    False):
                raise Exception("You can not activate both SmartRotateIpAddressMiddleware AND "
                                "RotateUserAgentAndIpAddressMiddleware or RotateIpAddressMiddleware! "
                                "Please check the spider attributes.")
        if self.enabled:
            if len(self.ips) <= 1:
                logger.error(
                    "[SMART_ROTATE_IP_MIDDLEWARE][SPIDER_OPENED][NEUKOLLN] Unable to use middleware properly "
                    "with only one single Ip address!!! (%s)" % self.ips[0])
                self.enabled = False  # unable to use middleware properly with only one single Ip address!!!
            else:
                # [INFO] Alert user enabled
                logger.info("[SMART_ROTATE_IP_MIDDLEWARE][SPIDER_OPENED][NEUKOLLN] is enabled")

    def process_request(self, request, spider):
        """Set the correct IP using the pool of Ips"""
        if self.enabled:
            request.meta['bindaddress'] = (self.get_random_ip_from_pool(), 0)
            request.headers['user-agent'] = self.get_user_agent_for_ip(request.meta['bindaddress'][0])

            # [INFO] Alert user
            logger.info("[SMART_ROTATE_IP_MIDDLEWARE][PROCESS_REQUEST][NEUKOLLN] "
                        "My IP is ==========> %s; My USER AGENT is ==========> %s; URL is ==========> %s"
                        % (str(request.meta['bindaddress']), str(request.headers['user-agent']), str(request.url)))

    def process_response(self, request, response, spider):
        """Interesting to check cached page"""

        # Check if Ip has been banned
        is_ban = getattr(spider, 'response_is_ban')
        ban = is_ban(request, response)

        # If Ip has been banned
        if ban:

            # If middleware is enable, remove Ip from pool and generate new request
            if self.enabled:
                banned_ip, port = request.meta['bindaddress']

                # Get url or redirected url
                url = request.meta.get('redirect_urls', [request.url])[0] if request.meta else request.url

                self.switch_ip(banned_ip, url, "PROCESS_RESPONSE")

                # [INFO] Alert user
                logger.info(
                    "[SMART_ROTATE_IP_MIDDLEWARE][PROCESS_RESPONSE][NEUKOLLN] Response has been detected as "
                    "banned; response.url: %s; retry this url: %s;" % (response.url, url))

                # Change Ip of the request, update url and meta and return request
                request.meta['bindaddress'] = (self.get_random_ip_from_pool(), 0)
                request.headers['user-agent'] = self.get_user_agent_for_ip(request.meta['bindaddress'][0])
                request.meta[NEUKOLLN_META_BANNED_KEY] = ban
                request.dont_filter = True  # DON'T FILTER!!!
                request = request.replace(url=url)

                return request

        # Return response and process it
        return response


class LuminatiRotateIpAddressMiddleware(NeukollnBaseMiddleware):

    #super_proxy = socket.gethostbyname('zproxy.luminati.io')
    #url = "http://%s-country-de-session-%s:%s@" + super_proxy + ":%d"
    #url = "http://%s-%s:%s@162.243.254.181:%d"
    url = "http://%s-session-%s:%s@162.243.254.181:%d"
    port = 22225

    def __init__(self):
        # [INFO] Alert user
        logger.info("[LUMINATI_ROTATE_IP_MIDDLEWARE][__INIT__][NEUKOLLN] Hello from middleware")

        # Credentials
        self._username = ""
        self._password = ""
        self._sessions_stack = []
        self._session_requests_limit = 10
        self._session_failures_limit = 2

        self.debug = False

        # Set list of user agent
        self.user_agents = get_project_settings().get(NEUKOLLN_SETTINGS_USER_AGENT_CHOICES)
        if not self.user_agents:
            # [DEBUG] Alert user
            logger.debug("[LUMINATI_ROTATE_IP_MIDDLEWARE][__INIT__][NEUKOLLN] %s is not set in the settings so a "
                         "default list of user agents from the library will be used." % NEUKOLLN_SETTINGS_USER_AGENT_CHOICES)
            self.user_agents = NEUKOLLN_USER_AGENT_DEFAULT_LIST

    def spider_opened(self, spider):
        if hasattr(spider, 'luminati_username'):
            # INFO it has to be the full username such as "lum-customer-autobiz-zone-autobiz_test_case-country-de-session"
            self._username = spider.luminati_username
        else:
            raise Exception("[LUMINATI_ROTATE_IP_MIDDLEWARE][SPIDER_OPENED][NEUKOLLN] Missing username (spider configuration)")

        if hasattr(spider, 'luminati_password'):
            self._password = spider.luminati_password
        else:
            raise Exception("[LUMINATI_ROTATE_IP_MIDDLEWARE][SPIDER_OPENED][NEUKOLLN] Missing password (spider configuration)")

        if hasattr(spider, 'luminati_allow_debug'):
            self.debug = spider.luminati_allow_debug
            if self.debug:
                logger.info("[LUMINATI_ROTATE_IP_MIDDLEWARE][SPIDER_OPENED][NEUKOLLN] Allow debug")

    def process_request(self, request, spider):
        """Set the correct IP using the pool of Ips"""

        # Retrieve session_id if exists else random e.g. reset session each time: 1 request = 1 proxy
        session_id = request.meta.get('luminati_session_id',random.random())

        # Random user agent
        j = int(session_id * len(self.user_agents))
        request.headers['user-agent'] = self.user_agents[j]

        # Set proxy
        _proxy = self.url % (self._username, session_id, self._password, self.port)
        request.meta['proxy'] = _proxy

        # Reset Proxy-Authorization
        try:
            del request.headers['Proxy-Authorization']
        except:
            pass

        # DEBUG ONLY - USE CAREFULLY - PLEASE NOT TOO MANY REQUESTS!!!
        if self.debug:
            proxy = urllib2.ProxyHandler({'http': _proxy})
            opener = urllib2.build_opener(proxy)
            res = opener.open('http://lumtest.com/myip.json').read()
            res = ast.literal_eval(res) if res else {}
            logger.info("[LUMINATI_ROTATE_IP_MIDDLEWARE][PROCESS_REQUEST][NEUKOLLN] Using IP ==========> %s, "
                        "Country ==========> %s, Proxy  ==========> %s" % (res.get('ip', 'Unknown'),
                                                                           res.get('country', 'Unknown'),
                                                                           _proxy))

    def process_response(self, request, response, spider):
        """Interesting to check cached page"""

        # Check if Ip has been banned
        is_ban = getattr(spider, 'response_is_ban')
        ban = is_ban(request, response)

        # If Ip has been banned
        if ban:

            # Get url or redirected url
            url = request.meta.get('redirect_urls', [request.url])[0] if request.meta else request.url

            # [INFO] Alert user
            logger.info(
                "[LUMINATI_ROTATE_IP_MIDDLEWARE][PROCESS_RESPONSE][NEUKOLLN] Response has been detected as banned; "
                "response.url: %s; retry this url: %s;" % (response.url, url))

            # Retrieve session_id if exists else random e.g. reset session each time: 1 request = 1 proxy
            session_id = request.meta.get('luminati_session_id', random.random())

            # Random user agent
            j = int(session_id * len(self.user_agents))
            request.headers['user-agent'] = self.user_agents[j]

            # Set proxy
            _proxy = self.url % (self._username, session_id, self._password, self.port)
            request.meta['proxy'] = _proxy

            # Reset Proxy-Authorization
            try:
                del request.headers['Proxy-Authorization']
            except:
                pass

            request.meta[NEUKOLLN_META_BANNED_KEY] = ban
            request.meta[NEUKOLLN_META_REFRESH_CACHE_KEY] = True # refresh cache!!!
            request.dont_filter = True  # DON'T FILTER!!!
            request = request.replace(url=url)

            return request

        # Return response and process it
        return response


class StormproxiesMiddleware(NeukollnBaseMiddleware):
    """Middleware tu use the service"""

    def __init__(self):
        # [INFO] Alert user
        logger.info("[STORMPROXIES_MIDDLEWARE][__INIT__][NEUKOLLN] Hello from middleware")

        # Init
        self._stormproxies_main_proxies = ""  # Main Proxies: 1 request = 1 IP

        # Set list of user agent
        self.user_agents = get_project_settings().get(NEUKOLLN_SETTINGS_USER_AGENT_CHOICES)
        if not self.user_agents:
            # [DEBUG] Alert userrand
            logger.debug("[STORMPROXIES_MIDDLEWARE][__INIT__][NEUKOLLN] %s is not set in the settings so a default "
                         "list of user agents from the library will be used." % NEUKOLLN_SETTINGS_USER_AGENT_CHOICES)
            self.user_agents = NEUKOLLN_USER_AGENT_DEFAULT_LIST

    def spider_opened(self, spider):
        if hasattr(spider, 'stormproxies_main_proxies'):
            self._stormproxies_main_proxies = spider.stormproxies_main_proxies
        else:
            raise Exception("[STORMPROXIES_MIDDLEWARE][SPIDER_OPENED][NEUKOLLN] "
                            "Missing stormproxies_main_proxies (spider configuration)")

    def set_proxy(self, request):

        # Retrieve session_id if exists else random e.g. reset session each time: 1 request = 1 proxy
        session_id = request.meta.get('stormproxies_session_id', random.random())

        # Random user agent
        j = int(session_id * len(self.user_agents))
        request.headers['user-agent'] = self.user_agents[j]
        request.meta['stormproxies_session_id'] = session_id

        if not request.meta.get('stormproxies_dont_use_proxy', False):
            # Set proxy
            i = int(session_id * len(self._stormproxies_main_proxies))
            _proxy = self._stormproxies_main_proxies[i]
            request.meta['proxy'] = _proxy

            # Reset Proxy-Authorization
            try:
                del request.headers['Proxy-Authorization']
            except:
                pass

        return request

    def process_request(self, request, spider):
        """Set the correct IP using the pool of Ips"""
        self.set_proxy(request)

    def process_response(self, request, response, spider):
        """Interesting to check cached page"""

        # Check if Ip has been banned
        is_ban = getattr(spider, 'response_is_ban')
        ban = is_ban(request, response)

        # If Ip has been banned
        if ban:

            # Get url or redirected url
            url = request.meta.get('redirect_urls', [request.url])[0] if request.meta else request.url

            # [INFO] Alert user
            logger.info("[STORMPROXIES_MIDDLEWARE][PROCESS_RESPONSE][NEUKOLLN] Response has been detected as banned; "
                        "response.url: %s; retry this url: %s;" % (response.url, url))

            request = self.set_proxy(request)
            request.meta[NEUKOLLN_META_BANNED_KEY] = ban
            request.meta[NEUKOLLN_META_REFRESH_CACHE_KEY] = True # refresh cache!!!
            request.dont_filter = True  # DON'T FILTER!!!
            request = request.replace(url=url)

            return request

        # Return response and process it
        return response
