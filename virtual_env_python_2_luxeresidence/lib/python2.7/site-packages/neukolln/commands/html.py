# -*- coding: utf-8 -*-

from __future__ import unicode_literals

import os, re
import gzip
from six.moves import cPickle as pickle
from time import time
from w3lib.http import headers_raw_to_dict
from scrapy.http import Headers
from scrapy.responsetypes import responsetypes
from scrapy.utils.project import get_project_settings
from scrapy.commands import ScrapyCommand
from scrapy.utils.response import open_in_browser
from scrapy.exceptions import UsageError


# Custom command based on the Scrapy command "view"
# Code on github: https://github.com/scrapy/scrapy/blob/master/scrapy/commands/view.py
# Documentation about the "view" command: https://doc.scrapy.org/en/latest/topics/commands.html#view


class Command(ScrapyCommand):

    requires_project = True

    def syntax(self):
        return "[options] <path>"

    def short_desc(self):
        return "Open cached page in browser"

    def long_desc(self):
        return "Open cached page in browser"

    # def add_options(self, parser):
    #     ScrapyCommand.add_options(self, parser)
    #     parser.add_option("--browser", dest="browser", help="use this browser")

    def run(self, args, opts):
        if len(args) != 1 or not os.path.exists(args[0]):
            raise UsageError()

        settings = get_project_settings()

        use_gzip = settings.getbool('HTTPCACHE_GZIP')
        _open = gzip.open if use_gzip else open

        expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')

        def _read_meta(spider, rpath):
            metapath = os.path.join(rpath, 'pickled_meta')
            if not os.path.exists(metapath):
                return  # not found
            mtime = os.stat(metapath).st_mtime
            if 0 < expiration_secs < time() - mtime:
                return  # expired
            with _open(metapath, 'rb') as f:
                return pickle.load(f)

        def retrieve_response(spider, rpath):
            """Return response if present in cache, or None otherwise."""
            metadata = _read_meta(spider, rpath)
            if metadata is None:
                return  # not cached
            with _open(os.path.join(rpath, 'response_body'), 'rb') as f:
                body = f.read()
            with _open(os.path.join(rpath, 'response_headers'), 'rb') as f:
                rawheaders = f.read()
            url = metadata.get('response_url')
            status = metadata['status']
            headers = Headers(headers_raw_to_dict(rawheaders))
            respcls = responsetypes.from_args(headers=headers, url=url)
            response = respcls(url=url, headers=headers, status=status, body=body)
            return response

        from neukolln.spiders import NeukollnBaseSpider

        spider = NeukollnBaseSpider()
        spider.crawl_date = re.search('.scrapy/[^/]+/([^/]+)/', args[0]).groups()[0]
        response = retrieve_response(spider, args[0])
        open_in_browser(response)